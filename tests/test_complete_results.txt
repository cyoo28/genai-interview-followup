Purpose: Testing the OpenAI API call in api_backend.py with a complete request
Payload:
{
  "question": "Can you describe a project where you implemented AI or machine learning to solve a real-world problem?",
  "answer": "I’ve been working on developing a consumer-facing chatbot product that leverages large language models. My focus has been on prompt engineering, designing structured prompts and leveraging tools like RAG to ensure the AI outputs are accurate and aligned with user expectations. I implemented proper guardrails using AWS Bedrock and Vertex AI, ensuring responses are filtered for unsafe content while maintaining a natural conversational flow.",
  "role": "AI Engineer",
  "interview_type": [
    "Technical",
    "Screening"
  ]
}

Follow-up: What evaluation metrics and tests did you use to measure accuracy, safety, and conversational quality of the chatbot?
Rationale: This reveals how you validated model performance and safety in a production setting.
Max Cosine Sim: 0.489
Status: PASSED

Follow-up: Can you describe the retrieval pipeline and knowledge sources used with RAG, and how you handled indexing, freshness, and relevance?
Rationale: This clarifies the architecture and data management decisions behind your RAG implementation.
Max Cosine Sim: 0.449
Status: PASSED

Follow-up: How did you implement guardrails in AWS Bedrock and Vertex AI—specifically the moderation models, enforcement logic, and any fallback or escalation paths?
Rationale: This helps evaluate your approach to safety engineering and runtime enforcement.
Max Cosine Sim: 0.756
Status: PASSED

