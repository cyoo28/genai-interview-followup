Purpose: Testing the OpenAI API call in api_backend.py with a minimal request (no optional arg)
Payload:
{
  "question": "Can you describe a project where you implemented AI or machine learning to solve a real-world problem?",
  "answer": "Iâ€™ve been working on developing a consumer-facing chatbot product that leverages large language models. My focus has been on prompt engineering, designing structured prompts and leveraging tools like RAG to ensure the AI outputs are accurate and aligned with user expectations. I implemented proper guardrails using AWS Bedrock and Vertex AI, ensuring responses are filtered for unsafe content while maintaining a natural conversational flow."
}

Follow-up: Can you describe the RAG architecture you used (retrieval method, vector store, indexing, and latency considerations) and how it improved response accuracy?
Rationale: This clarifies the technical choices behind retrieval and their impact on answer quality and performance.
Max Cosine Sim: 0.491
Status: PASSED

Follow-up: What metrics and evaluation process did you use to measure alignment, accuracy, and safety of the chatbot, and how did you iterate based on those results?
Rationale: This probes how you validated model behavior and used feedback to improve the system.
Max Cosine Sim: 0.498
Status: PASSED

Follow-up: How did you implement guardrails in AWS Bedrock and Vertex AI (e.g., classifiers, policy engines, prompt filters), and how did you balance safety with conversational naturalness?
Rationale: This asks for concrete implementation details and the trade-offs made between safety controls and user experience.
Max Cosine Sim: 0.793
Status: PASSED

