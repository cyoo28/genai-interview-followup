Purpose: Testing the OpenAI API call in api_backend.py with a complete request
Payload:
{
  "question": "Can you describe a project where you implemented AI or machine learning to solve a real-world problem?",
  "answer": "I’ve been working on developing a consumer-facing chatbot product that leverages large language models. My focus has been on prompt engineering, designing structured prompts and leveraging tools like RAG to ensure the AI outputs are accurate and aligned with user expectations. I implemented proper guardrails using AWS Bedrock and Vertex AI, ensuring responses are filtered for unsafe content while maintaining a natural conversational flow.",
  "role": "AI Engineer",
  "interview_type": [
    "Technical",
    "Screening"
  ]
}

Follow-up: Can you describe the RAG pipeline you implemented — choice of vector DB, embedding model, chunking strategy, and how you handle retrieval relevance and latency?
Rationale: This clarifies the technical choices and trade-offs that affect retrieval accuracy and system performance.
Max Cosine Sim: 0.493
Status: PASSED

Follow-up: What specific prompt engineering patterns or templates did you design, and how did you measure their impact on accuracy, coherence, and user satisfaction?
Rationale: This probes the concrete prompt techniques used and how you validated their effectiveness with metrics or user feedback.
Max Cosine Sim: 0.616
Status: PASSED

Follow-up: How did you implement and enforce guardrails across AWS Bedrock and Vertex AI (e.g., pre/post-processing, model filters, real-time monitoring), and how do you handle false positives that degrade UX?
Rationale: This asks for operational details on safety controls and how you manage errors or user experience trade-offs.
Max Cosine Sim: 0.727
Status: PASSED

